<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="TGVAC">
  <meta property="og:title" content="TGVAC"/>
  <meta property="og:description" content="TGVAC"/>
  <meta property="og:url" content="https://uark-aicv.github.io/TextGuidedVAC/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TGVAC">
  <meta name="twitter:description" content="TGVAC">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="TGVAC">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Text-Guided Video Amodal Completion</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Text-Guided Video Amodal Completion</h1>
            <div class="is-size-5 publication-authors">
                <!-- Paper authors -->
              <span class="author-block">
                <a href="https://trqminh.github.io/" target="_blank">Minh Tran</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://uark-aicv.github.io/TextGuidedVAC/" target="_blank">Winston Bounsavy</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://uark-aicv.github.io/TextGuidedVAC/" target="_blank">Taisei Hanyu</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://uark-aicv.github.io/TextGuidedVAC/" target="_blank">Thang Pham</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://vhvkhoa.github.io/" target="_blank">Khoa Vo</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://uark-aicv.github.io/TextGuidedVAC/" target="_blank">Tri Nguyen</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://uark-aicv.github.io/" target="_blank">Ngan Le</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>University of Arkansas</span>&nbsp;&nbsp;&nbsp;
              <span class="author-block"><sup>2</sup>Coupang, Inc.</span>
              <br>
              <span class="author-block">Preprint 2024</span>
            </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://trqminh.github.io/pdfs/vac.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/UARK-AICV/TextGuidedVAC" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/videos/demo3.gif" alt="MY ALT TEXT"/>
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/demo3.mp4"
        type="video/mp4">
      </video> -->
      <h2 class="subtitle has-text-centered">
        Text-Guided Video Amodal Completion. Given an input video, users select an object of interest in the first frame and provide a text description of the expected output. Our pipeline then generates a completed video, filling in the missing shape and texture of the object. 
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Network -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <img src="static/images/VAC/teaser.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          An overview of our text-guided video amodal completion pipeline. Given an
input video, users select an object of interest in the first frame and provide a text descrip-
tion of the expected output. Our pipeline then generates a completed video, filling in the
missing shape and texture of the object. (Top) A summary of our proposed pipeline and
three key contributions. (Bottom) Zero-shot transfer results on natural videos using our
method.
       </h2>
      </div>
    </div>
  </div>
</section>



<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Amodal perception enables humans to perceive entire objects even when parts are occluded, a remarkable cognitive skill that artificial intelligence struggles to replicate. While substantial advancements have been made in image amodal completion, video amodal completion remains underexplored despite its high potential for real-world applications in video editing and analysis. In response, we propose a video amodal completion framework to explore this potential direction. Our contributions include (i) a synthetic dataset for video amodal completion with text description for the object of interest. The dataset captures a variety of object types, textures, motions, and scenarios to support zero-shot transferring on natural videos. (ii) A diffusion-based text-guided video amodal completion framework enhanced with a motion continuity module to ensure temporal consistency across frames. (iii) Zero-shot inference for long video, inspired by temporal diffusion techniques to effectively manage long video sequences while improving inference accuracy and maintaining coherent amodal completions. Experimental results shows the efficacy of our approach in handling video amodal completion, opening potential capabilities for advanced video editing and analysis with amodal completion.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->




<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Training pipeline of the proposed method</h2>
        <img src="static/images/VAC/pipeline-1.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Our approach follows a common two-stage training strategy: first, training a denoising UNet at the frame level to
capture spatial features, and then incorporating motion training to ensure temporal coherence.
        </h2>
        <!-- <img src="static/images/VAC/example_qualitative.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          [Sample Text]
        </h2> -->
      </div>
    </div>
  </div>
</section>


<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <img src="static/images/aisformer/aistr-Encoder-Decoder/aistr-Encoder-Decoder-1.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Illustration network architecture of AISFormer. (a): mask transformer encoder is designed as one block of self-attention, (b): mask transformer decoder is designed as a combination of one block of self-attention and one block of cross-attention and (c): invisible embedding is designed as an MLP with two hidden layers. 
        </h2>
      </div>
  </div>
</div>
</div>
</section> -->

<!-- Network -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Synthesizing training data process</h2>
        <img src="static/images/VAC/dataset_creation-1.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          We selected videos with unoccluded
objects and used provided masks to isolate object pixels. These were used to synthesize
occlusion videos and create corresponding amodal completion ground truth. A vision
language model (e.g. BLIP is utilized to generate ground truth’s text description.)
        </h2>
      </div>
    </div>
  </div>
</section>


<!-- Qualitative Results -->
<section class="hero is-small">
  <div class="hero-body has-text-centered">
    <h2 class="title is-3">Qualitative Results</h2>
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/VAC/example_easy_case_2.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Qualitative results of our model compared to ProgressiveAmodal and Pix2gestalt
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/VAC/example_hard_case_2.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Qualitative results of our model compared to ProgressiveAmodal and Pix2gestalt
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/VAC/example_extra_02.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Qualitative results of our model compared to ProgressiveAmodal and Pix2gestalt
       </h2>
     </div>
     <!-- <div class="item">
      <img src="static/images/aisformer/AttnVis/AttnVis-1.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Attention map visualizations indicating the cross attention between our learnable queries and the corresponding region of interest.
      </h2>
    </div> -->
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!-- Zero-shot Results -->
<section class="hero is-light">
  <div class="hero-body has-text-centered">
    <h2 class="title is-3">Zero Shot Performance</h2>
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/VAC/zero_shot_1_resized.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Zero-shot amodal completion on natural videos
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/VAC/zero_shot_2_resized.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Zero-shot amodal completion on natural videos
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/VAC/zero_shot_3_resized.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Zero-shot amodal completion on natural videos
       </h2>
     </div>
     <!-- <div class="item">
      <img src="static/images/aisformer/AttnVis/AttnVis-1.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Attention map visualizations indicating the cross attention between our learnable queries and the corresponding region of interest.
      </h2>
    </div> -->
  </div>
</div>
</div>
</section>
<!-- End image carousel -->




<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code></code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
    <!-- <h2 class="title">Acknowledgement</h2> -->
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This work is sponsored by National Science Foundation (NSF) under Award No OIA-1946391 RII Track-1 Honors College Research Award, University of Arkansas,
NSF 2223793 EFRI BRAID
             <br><br>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page. This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
